

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sktime.regressors.tsf &mdash; sktime  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> sktime
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extension.html">Extension Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autogen/sktime.html">sktime package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">sktime</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sktime.regressors.tsf</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sktime.regressors.tsf</h1><div class="highlight"><pre>
<span></span><span class="n">__author__</span> <span class="o">=</span> <span class="s2">&quot;Markus LÃ¶ning&quot;</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;TimeSeriesForestRegressor&quot;</span><span class="p">]</span>


<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">warn</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">issparse</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.base</span> <span class="kn">import</span> <span class="n">_partition_estimators</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.forest</span> <span class="kn">import</span> <span class="n">ForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble.forest</span> <span class="kn">import</span> <span class="n">MAX_INT</span>
<span class="kn">from</span> <span class="nn">sklearn.exceptions</span> <span class="kn">import</span> <span class="n">DataConversionWarning</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree._tree</span> <span class="kn">import</span> <span class="n">DOUBLE</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">sklearn.utils._joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span>

<span class="kn">from</span> <span class="nn">sktime.classifiers.compose.ensemble</span> <span class="kn">import</span> <span class="n">_parallel_build_trees</span>
<span class="kn">from</span> <span class="nn">sktime.transformers.summarise</span> <span class="kn">import</span> <span class="n">RandomIntervalFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">sktime.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sktime.utils.time_series</span> <span class="kn">import</span> <span class="n">time_series_slope</span>
<span class="kn">from</span> <span class="nn">sktime.utils.validation.supervised</span> <span class="kn">import</span> <span class="n">validate_X</span><span class="p">,</span> <span class="n">validate_X_y</span>


<div class="viewcode-block" id="TimeSeriesForestRegressor"><a class="viewcode-back" href="../../../autogen/sktime.regressors.tsf.html#sktime.regressors.tsf.TimeSeriesForestRegressor">[docs]</a><span class="k">class</span> <span class="nc">TimeSeriesForestRegressor</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Time-Series Forest Regressor.</span>

<span class="sd">    A time series forest is a meta estimator and an adaptation of the random forest</span>
<span class="sd">    for time-series/panel data that fits a number of decision tree classifiers on</span>
<span class="sd">    various sub-samples of a transformed dataset and uses averaging to improve the</span>
<span class="sd">    predictive accuracy and control over-fitting. The sub-sample size is always the same as the original</span>
<span class="sd">    input sample size but the samples are drawn with replacement if `bootstrap=True` (default).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : Pipeline</span>
<span class="sd">        A pipeline consisting of series-to-tabular transformers</span>
<span class="sd">        and a decision tree classifier as final estimator.</span>
<span class="sd">    n_estimators : integer, optional (default=100)</span>
<span class="sd">        The number of trees in the forest.</span>
<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion and minimizes the L2 loss</span>
<span class="sd">        using the mean of each terminal node, &quot;friedman_mse&quot;, which uses mean</span>
<span class="sd">        squared error with Friedman&#39;s improvement score for potential splits,</span>
<span class="sd">        and &quot;mae&quot; for the mean absolute error, which minimizes the L1 loss</span>
<span class="sd">        using the median of each terminal node.</span>
<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>
<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>
<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>
<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>
<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)` (same as &quot;auto&quot;).</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>
<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>
<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>
<span class="sd">    min_impurity_split : float, (default=1e-7)</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>
<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>
<span class="sd">    oob_score : bool (default=False)</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>
<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>
<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>
<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>
<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>
<span class="sd">    None, optional (default=None)</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>
<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>
<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>
<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>
<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>
<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>
<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>
<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>
<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>
<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">    oob_decision_function_ : array of shape = [n_samples, n_classes]</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">,</span> <span class="n">time_series_slope</span><span class="p">]</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;transform&#39;</span><span class="p">,</span> <span class="n">RandomIntervalFeatureExtractor</span><span class="p">(</span><span class="n">n_intervals</span><span class="o">=</span><span class="s1">&#39;sqrt&#39;</span><span class="p">,</span> <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">)),</span>
                     <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">())]</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>

        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">Pipeline</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Base estimator must be pipeline with transforms.&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">DecisionTreeRegressor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Last step in base estimator pipeline must be DecisionTreeRegressor.&#39;</span><span class="p">)</span>

        <span class="c1"># Assign values, even though passed on to base estimator below, necessary here for cloning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>

        <span class="c1"># Rename estimator params according to name in pipeline.</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">estimator_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;criterion&quot;</span><span class="p">:</span> <span class="n">criterion</span><span class="p">,</span>
            <span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="n">max_depth</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_split&quot;</span><span class="p">:</span> <span class="n">min_samples_split</span><span class="p">,</span>
            <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">:</span> <span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">:</span> <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="s2">&quot;max_features&quot;</span><span class="p">:</span> <span class="n">max_features</span><span class="p">,</span>
            <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">:</span> <span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">:</span> <span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">:</span> <span class="n">min_impurity_split</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">estimator_params</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{estimator}</span><span class="s1">__</span><span class="si">{pname}</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">pval</span> <span class="k">for</span> <span class="n">pname</span><span class="p">,</span> <span class="n">pval</span> <span class="ow">in</span> <span class="n">estimator_params</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

        <span class="c1"># Pass on params.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TimeSeriesForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">estimator_params</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Assign random state to pipeline.</span>
        <span class="n">base_estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s1">&#39;random_state&#39;</span><span class="p">:</span> <span class="n">random_state</span><span class="p">,</span> <span class="s1">&#39;check_input&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>

        <span class="c1"># Store renamed estimator params.</span>
        <span class="k">for</span> <span class="n">pname</span><span class="p">,</span> <span class="n">pval</span> <span class="ow">in</span> <span class="n">estimator_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">pname</span><span class="p">,</span> <span class="n">pval</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_input</span> <span class="o">=</span> <span class="n">check_input</span>

<div class="viewcode-block" id="TimeSeriesForestRegressor.fit"><a class="viewcode-back" href="../../../autogen/sktime.regressors.tsf.html#sktime.regressors.tsf.TimeSeriesForestRegressor.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a forest of trees from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted</span>
<span class="sd">            to ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>
<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>
<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validate_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Validate or convert input data</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="c1"># Remap output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;A column-vector y was passed when a 1d array was&quot;</span>
                 <span class="s2">&quot; expected. Please change the shape of y to &quot;</span>
                 <span class="s2">&quot;(n_samples,), for example using ravel().&quot;</span><span class="p">,</span>
                 <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
            <span class="c1"># [:, np.newaxis] that does not.</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>

        <span class="c1"># Check parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available&quot;</span>
                             <span class="s2">&quot; if bootstrap=True&quot;</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="c1"># Free allocated memory, if any</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                             <span class="s1">&#39;len(estimators_)=</span><span class="si">%d</span><span class="s1"> when warm_start==True&#39;</span>
                             <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>

        <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
                 <span class="s2">&quot;fit new trees.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># We draw from the random state to get the random state we</span>
                <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
                <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>

            <span class="n">trees</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">)]</span>

            <span class="c1"># Parallel loop: for standard random forests, the threading</span>
            <span class="c1"># backend is preferred as the Cython code for fitting the trees</span>
            <span class="c1"># is internally releasing the Python GIL making threading more</span>
            <span class="c1"># efficient than multiprocessing in that case. However, in this case,</span>
            <span class="c1"># for fitting pipelines in parallel, multiprocessing is more efficient.</span>
            <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                    <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">))</span>

            <span class="c1"># Collect newly grown trees</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_oob_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="TimeSeriesForestRegressor.predict"><a class="viewcode-back" href="../../../autogen/sktime.regressors.tsf.html#sktime.regressors.tsf.TimeSeriesForestRegressor.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>
<span class="sd">        The predicted regression target of an input sample is computed as the</span>
<span class="sd">        mean predicted regression targets of the trees in the forest.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="c1"># Check data</span>
        <span class="n">validate_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># Parallel loop</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">predict</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate_X_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of features of the model must &quot;</span>
                             <span class="s2">&quot;match the input. Model n_features is </span><span class="si">%s</span><span class="s2"> and &quot;</span>
                             <span class="s2">&quot;input n_features is </span><span class="si">%s</span><span class="s2"> &quot;</span>
                             <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">X</span>

<div class="viewcode-block" id="TimeSeriesForestRegressor.apply"><a class="viewcode-back" href="../../../autogen/sktime.regressors.tsf.html#sktime.regressors.tsf.TimeSeriesForestRegressor.apply">[docs]</a>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

<div class="viewcode-block" id="TimeSeriesForestRegressor.decision_path"><a class="viewcode-back" href="../../../autogen/sktime.regressors.tsf.html#sktime.regressors.tsf.TimeSeriesForestRegressor.decision_path">[docs]</a>    <span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, The Alan Turing Institute

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>